{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solsa24/AI-Chat-Assistant/blob/master/AI_Agent_with_Gemini_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import time\n",
        "\n",
        "# --- Part 0: Configure the LLM ---\n",
        "try:\n",
        "    # Get the API key from Codespaces secrets\n",
        "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"GEMINI_API_KEY not found. Please set it in your Codespaces secrets.\")\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"Gemini API configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- Part 1: MCP (Modular Code Package) Builder ---\n",
        "\n",
        "class MCPBuilder:\n",
        "    \"\"\"\n",
        "    Builds, reuses, and manages Modular Code Packages (MCPs).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self._registry = {}\n",
        "\n",
        "    def create_mcp(self, name: str, description: str, backend_tech: str, frontend_tech: str):\n",
        "        \"\"\"Creates a new MCP and adds it to the registry.\"\"\"\n",
        "        if name in self._registry:\n",
        "            print(f\"MCP '{name}' already exists. Reusing.\")\n",
        "            return self._registry[name]\n",
        "\n",
        "        mcp = {\n",
        "            \"name\": name,\n",
        "            \"description\": description,\n",
        "            \"backend\": {\"tech\": backend_tech, \"files\": []},\n",
        "            \"frontend\": {\"tech\": frontend_tech, \"files\": []},\n",
        "        }\n",
        "        self._registry[name] = mcp\n",
        "        print(f\"MCP '{name}' created.\")\n",
        "        return mcp\n",
        "\n",
        "    def get_mcp(self, name: str):\n",
        "        \"\"\"Retrieves an MCP from the registry.\"\"\"\n",
        "        return self._registry.get(name)\n",
        "\n",
        "# --- Part 2: Code Generator ---\n",
        "\n",
        "class CodeGenerator:\n",
        "    \"\"\"\n",
        "    Generates code by calling the Gemini LLM.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Initialize the Gemini model\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "        print(\"Gemini 1.5 Pro model initialized.\")\n",
        "\n",
        "    def _llm_generate_code(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Calls the Gemini API to generate code.\n",
        "        Includes error handling and retries.\n",
        "        \"\"\"\n",
        "        print(f\"  - Sending prompt to Gemini: '{prompt[:50]}...'\")\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            # Add a small delay to respect API rate limits\n",
        "            time.sleep(2)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"  - An error occurred with the Gemini API: {e}\")\n",
        "            return f\"# Error generating code: {e}\"\n",
        "\n",
        "    def generate_backend(self, mcp: dict):\n",
        "        \"\"\"Generates backend code for a given MCP.\"\"\"\n",
        "        print(f\"\\nGenerating {mcp['backend']['tech']} backend for {mcp['name']}...\")\n",
        "\n",
        "        main_py_prompt = f\"Generate a complete, runnable main.py file for a FastAPI application for '{mcp['description']}'. Include basic CRUD endpoints for recipes. The code should be fully functional.\"\n",
        "        models_py_prompt = f\"Generate a models.py file for a FastAPI application for '{mcp['description']}'. It should include a Pydantic model for a 'Recipe' with fields: name (str), ingredients (list[str]), and instructions (str).\"\n",
        "\n",
        "        main_py_code = self._llm_generate_code(main_py_prompt)\n",
        "        models_py_code = self._llm_generate_code(models_py_prompt)\n",
        "\n",
        "        mcp[\"backend\"][\"files\"] = [\n",
        "            {\"name\": \"main.py\", \"content\": main_py_code},\n",
        "            {\"name\": \"models.py\", \"content\": models_py_code},\n",
        "        ]\n",
        "        print(f\"Backend for {mcp['name']} generated.\")\n",
        "        return mcp\n",
        "\n",
        "    def generate_frontend(self, mcp: dict):\n",
        "        \"\"\"Generates frontend code for a given MCP.\"\"\"\n",
        "        print(f\"\\nGenerating {mcp['frontend']['tech']} frontend for {mcp['name']}...\")\n",
        "\n",
        "        index_js_prompt = f\"Generate a complete, runnable pages/index.js file for a Next.js frontend for '{mcp['description']}'. It should be a simple landing page that displays a title and a brief welcome message. Use functional components and basic JSX.\"\n",
        "\n",
        "        index_js_code = self._llm_generate_code(index_js_prompt)\n",
        "\n",
        "        mcp[\"frontend\"][\"files\"] = [\n",
        "            {\"name\": \"pages/index.js\", \"content\": index_js_code}\n",
        "        ]\n",
        "        print(f\"Frontend for {mcp['name']} generated.\")\n",
        "        return mcp\n",
        "\n",
        "# --- Part 3: Orchestrator ---\n",
        "\n",
        "class AIAgent:\n",
        "    \"\"\"\n",
        "    The main AI Agent that orchestrates the app creation process.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mcp_builder = MCPBuilder()\n",
        "        self.code_generator = CodeGenerator()\n",
        "\n",
        "    def create_app(self, description: str, backend_tech: str, frontend_tech: str):\n",
        "        \"\"\"\n",
        "        The one-command function to generate the full-stack application.\n",
        "        \"\"\"\n",
        "        print(\"--- Starting AI Agent for Full-Stack App Development ---\")\n",
        "\n",
        "        app_mcp = self.mcp_builder.create_mcp(\n",
        "            name=\"RecipeAppCore\",\n",
        "            description=description,\n",
        "            backend_tech=backend_tech,\n",
        "            frontend_tech=frontend_tech,\n",
        "        )\n",
        "\n",
        "        self.code_generator.generate_backend(app_mcp)\n",
        "        self.code_generator.generate_frontend(app_mcp)\n",
        "\n",
        "        self._save_code(app_mcp)\n",
        "\n",
        "        print(\"\\n--- ✅ Full-Stack App Generation Complete! ---\")\n",
        "        print(\"\\nGenerated file structure:\")\n",
        "        print(\"\"\"\n",
        "recipe_app/\n",
        "├── backend/\n",
        "│   ├── main.py\n",
        "│   └── models.py\n",
        "└── frontend/\n",
        "    └── pages/\n",
        "        └── index.js\n",
        "\"\"\")\n",
        "        print(\"\\nTo explore the files, check the file explorer on the left.\")\n",
        "        print(\"To test the backend, you would typically run: 'cd recipe_app/backend && pip install fastapi uvicorn && uvicorn main:app --reload'\")\n",
        "\n",
        "    def _save_code(self, mcp: dict):\n",
        "        \"\"\"Saves the generated code to the filesystem.\"\"\"\n",
        "        base_dir = \"recipe_app\"\n",
        "        # Clean up old directory if it exists\n",
        "        if os.path.exists(base_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(base_dir)\n",
        "\n",
        "        backend_dir = os.path.join(base_dir, \"backend\")\n",
        "        frontend_dir = os.path.join(base_dir, \"frontend\", \"pages\")\n",
        "\n",
        "        os.makedirs(backend_dir, exist_ok=True)\n",
        "        os.makedirs(frontend_dir, exist_ok=True)\n",
        "\n",
        "        for file_info in mcp[\"backend\"][\"files\"]:\n",
        "            # Clean up markdown code blocks if the LLM includes them\n",
        "            content = file_info[\"content\"].replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
        "            with open(os.path.join(backend_dir, file_info[\"name\"]), \"w\") as f:\n",
        "                f.write(content)\n",
        "\n",
        "        for file_info in mcp[\"frontend\"][\"files\"]:\n",
        "            content = file_info[\"content\"].replace(\"```javascript\", \"\").replace(\"```jsx\", \"\").replace(\"```\", \"\").strip()\n",
        "            with open(os.path.join(frontend_dir, os.path.basename(file_info[\"name\"])), \"w\") as f:\n",
        "                f.write(content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = AIAgent()\n",
        "    agent.create_app(\n",
        "        description=\"A recipe web app with user login, recipe search by ingredients, social sharing, and ratings\",\n",
        "        backend_tech=\"FastAPI\",\n",
        "        frontend_tech=\"Next.js\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "D43bjjot7IQk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}